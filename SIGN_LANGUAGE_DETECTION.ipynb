{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign Language Detection - I\n",
    "# Saving Data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import csv\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hand = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hand = mp_hand.Hands(min_detection_confidence=0.7,min_tracking_confidence=0.7)\n",
    "\n",
    "if not os.path.exists(\"asl_dataset\"):\n",
    "    os.makedirs(\"asl_dataset\")\n",
    "\n",
    "file_path = \"asl_dataset/landmarks.csv\"\n",
    "if not os.path.isfile(file_path):\n",
    "    with open(file_path,'w',newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([f\"x{i}\" for i in range(21)] + [f\"y{i}\" for i in range(21)] + [\"label\"])\n",
    "\n",
    "print(\"Press keys A-Z to record samples. Press 'd' to Stop.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    r, f = cap.read()\n",
    "    if r == False:\n",
    "        break\n",
    "    f = cv2.flip(f, 1)\n",
    "    rgb = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    res = hand.process(rgb)\n",
    "    key = cv2.waitKey(1) & 0xff\n",
    "\n",
    "    if res.multi_hand_landmarks:\n",
    "        for landmarks in res.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(f, landmarks, mp_hand.HAND_CONNECTIONS)\n",
    "\n",
    "            x_list = []\n",
    "            y_list = []\n",
    "            for lm in landmarks.landmark:\n",
    "                h, w, _ = f.shape\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                x_list.append(x)\n",
    "                y_list.append(y)\n",
    "\n",
    "            if 97 <= key <= 122:\n",
    "                label = chr(key).upper()\n",
    "                print(f\"Saving label for : {label}\")\n",
    "                with open(file_path, 'a', newline='') as f_csv:\n",
    "                    writer = csv.writer(f_csv)\n",
    "                    writer.writerow(x_list + y_list + [label])\n",
    "\n",
    "    if key == ord('d'):\n",
    "        # Optionally, you can print a message when stopping\n",
    "        print(\"Stopped recording samples.\")\n",
    "        break\n",
    "\n",
    "    # Only show the frame if it is valid\n",
    "    if f is not None and isinstance(f, np.ndarray):\n",
    "        cv2.imshow(\"Sign Language Detection\", f)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign Language Detection - II\n",
    "# Training Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def normalize_landmarks(df):\n",
    "    x_cols = [f\"x{i}\" for i in range(21)]\n",
    "    y_cols = [f\"y{i}\" for i in range(21)]\n",
    "\n",
    "    for i in range(21):\n",
    "        df[f\"x{i}\"] = df[f\"x{i}\"] - df[\"x0\"]\n",
    "        df[f\"y{i}\"] = df[f\"y{i}\"] - df[\"y0\"]\n",
    "    \n",
    "    return df.drop([\"x0\", \"y0\"], axis=1)\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"asl_dataset/landmarks.csv\")\n",
    "\n",
    "x = data.drop(\"label\",axis=1)\n",
    "y = data[\"label\"]\n",
    "features = data.drop(\"label\", axis=1)\n",
    "features.columns = [f\"x{i}\" if i < 21 else f\"y{i-21}\" for i in range(42)]  # Rename columns\n",
    "\n",
    "features = normalize_landmarks(features)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split(features,y_encoded,test_size=0.2,random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "# {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "model = RandomForestClassifier(n_estimators=200,max_depth=10,min_samples_split=2,random_state=42)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "ac = accuracy_score(y_test,y_pred)\n",
    "print(\"Total Accuracy: \",ac)\n",
    "print(\"Classification report :\\n\" ,classification_report(y_test,y_pred))\n",
    "\n",
    "joblib.dump(model,\"asl_model.pkl\")\n",
    "joblib.dump(le,\"label_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a80003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [10, 20, None],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, verbose=2)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac805158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign Language Detection - III\n",
    "# Visualizing the Trained Data\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import joblib\n",
    "\n",
    "model = joblib.load(\"asl_model.pkl\")\n",
    "le = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hand = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hand.Hands(min_detection_confidence=0.8,min_tracking_confidence=0.8)\n",
    "\n",
    "while cap.isOpened():\n",
    "    r,f = cap.read()\n",
    "    if r==False:\n",
    "        break\n",
    "    f = cv2.flip(f,1)\n",
    "    rgb = cv2.cvtColor(f,cv2.COLOR_BGR2RGB)\n",
    "    res = hands.process(rgb)\n",
    "\n",
    "    if res.multi_hand_landmarks:\n",
    "        for landmarks in res.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(f,landmarks,mp_hand.HAND_CONNECTIONS)\n",
    "\n",
    "            h,w,_ = f.shape\n",
    "            x_list,y_list = [],[]\n",
    "            for lm in landmarks.landmark:\n",
    "                x_list.append(int(lm.x*w))\n",
    "                y_list.append(int(lm.y*h))\n",
    "\n",
    "            if(len(x_list)==21 and len(y_list)==21):\n",
    "                # sample = np.array([x_list+y_list])\n",
    "                x0, y0 = x_list[0], y_list[0]\n",
    "                x_norm = [x - x0 for x in x_list]\n",
    "                y_norm = [y - y0 for y in y_list]\n",
    "                x_norm = x_norm[1:]\n",
    "                y_norm = y_norm[1:]\n",
    "                sample = np.array([x_norm + y_norm])  # 40 features as expected\n",
    "\n",
    "                # print(sample,\"\\n\\n\\n\\n\")\n",
    "                prediction = model.predict(sample)\n",
    "                label = le.inverse_transform(prediction)[0]\n",
    "                cv2.putText(f,f\"Prediction: {label}\",(10,40),cv2.FONT_HERSHEY_COMPLEX_SMALL,1,(0,255,0),1)\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Sign Language\",f)\n",
    "    if cv2.waitKey(25) & 0xff == ord('d'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
